{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66919895",
   "metadata": {},
   "source": [
    "# **W281_Fall_2022_Final_Report_Driver_Behavior_Detection**\n",
    "## by Hoon Kim, Kai Ying, Ram Senthamarai, Dmitry Baron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d68ff",
   "metadata": {},
   "source": [
    "## Overview\n",
    "### Goal\n",
    "CDC reported that 3,100 were killed and 424,000 were injured in crashes involving a distracted driver in 2019. While dashcam has become a common and mature technology, if we could use the dashcam videos/images captured to classify various distracted driver behaviors, we could help develop assistive tools to reduce the risk of driver distraction and hence improve public driving safety. \n",
    "\n",
    "### Problem Statement\n",
    "The main objective of our project is to design classification algorithm to classify ten types of driver behavior via identification of the differences in driver’s facial and posture when performing each of these actions: 1) Safe Driving, 2) Texting (right), 3) Phone Call (right), 4) Texting(left), 5) Phone Call(left), 6) Fiddling With Console, 7) Drinking, 8) Reaching Back, 9) Fixing Looks, 10) Conversing.\n",
    "\n",
    "### Dataset\n",
    "The dataset is provided from a [Kaggle competition](https://www.kaggle.com/competitions/state-farm-distracted-driver-detection/data) hosted by State Farms. Kaggle provides both the training and testing dataset. However, given that the testing dataset does not have any class label associated with them, for the purpose of this class project, we decided to exclude the test set given.\n",
    "\n",
    "The training dataset consists of a total of 22,424 images captured in a fixed angle from an in-car dashcam. These images are classified into 10 types of driving behavior as mentioned above. Each image is a 640 x 480 JPEG image with RGB color space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25166f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Config & Library Imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import time\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib\n",
    "\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import transformers\n",
    "import eda_helpers\n",
    "import feature_helpers\n",
    "import viz\n",
    "import configuration\n",
    "import customdataset\n",
    "import enums\n",
    "\n",
    "device = 'cpu'\n",
    "config = configuration.Configuration()\n",
    "face_config = configuration.FaceConfig(config)\n",
    "pose_config = configuration.PoseConfig(config)\n",
    "vizualizer = viz.Vizualizer(config, face_config, pose_config, tqdm=tqdm)\n",
    "feature_extractor = feature_helpers.FeatureExtractor(config, face_config, pose_config, tqdm)\n",
    "\n",
    "IMAGE_TYPES = [enums.ImageTypes.ORIGINAL, enums.ImageTypes.POSE, enums.ImageTypes.FACE]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Training Data\n",
    "vizualizer.plot_raw_class_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb42bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Sample Images\n",
    "vizualizer.plot_samples(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e605af1",
   "metadata": {},
   "source": [
    "Given the large number of training data and the limited computing resources we have, we reduced the number of data by sampling 600 samples in each of the ten classes that the driver’s face is detectable using a MTCNN classifier. We then separated our dataset into train, validation, and test data using a 80-10-10 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8012f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract face from each image. Takes a few hours to run.\n",
    "def extract_faces():\n",
    "    eda_helpers.FaceExtractor(config, tqdm).extract_faces(face_config.FEATURES_FOLDER, face_config.FACE_SUMMARY_NAME, config.ANNOTATION_FILE)\n",
    "# extract_faces() # Commented out as this takes a long time to run on our 20,000+ images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6fcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of Face Identification\n",
    "vizualizer.plot_faces_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of face detection.\n",
    "vizualizer.display_faces(per_group_count = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, test\n",
    "def split_dataset():\n",
    "    splitter = eda_helpers.SampleSplitter(config, face_config, pose_config, tqdm=tqdm)\n",
    "    splitter.sample(config.class_dict.keys(), samples_per_class=[600, 480, 60, 60], out_file=config.ANNOTATION_FILE)\n",
    "\n",
    "# split_dataset() # We have commented this code out as this is an one time task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5411f9e",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "### Facial, Hand, Torso Keypoint Position Features\n",
    "#### Intuition\n",
    "Many of the behaviors in our classes could be distinguished by the difference in the driver’s head orientation, hand position, and body orientation. For example, fiddling with the center console usually involves a driver stretching their right arm and having their head slightly lower and towards the right, trying to press buttons on the console.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d34347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image of Fiddling With Console (c6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8d243",
   "metadata": {},
   "source": [
    "Since camera angle is fixed, the angle of the driver’s orientation is relatively stable in the image throughout each class. Hence, the relative position of the body parts detected is stable across each image if the driver is behaving the same way. This allows us to capture differences more due to the different behavior and less of the nose introduced by different camera angles.\n",
    "\n",
    "For driver facial orientation, we rely on key points of the driver’s eyes, noses, and lips. From the passenger seat perspective, the tighter the nose, lips, and eyes are located in the image of the driver would indicate a higher chance that the driver is facing front as one of the eyes and parts of the noses and lips would not be clearly visible in the image, and vice versa.\n",
    "\n",
    "For hand position and body orientation, we rely on key points of the drivers’ shoulder, elbow, wrist, and hip. From these key points, we could roughly draw out the body posture of the driver’s arm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b2f85a",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "Each image was resized and padded with 0 pixel values on the edges to ensure it keeps the aspect ratio while also fitting the expected size for our human pose estimation model. \n",
    "Then, we passed each image into the Tensorflow MoveNet model to do keypoint detection. [link](https://www.tensorflow.org/hub/tutorials/movenet)\n",
    "The model outputted the coordinates and the associated scores of each key point detected.\n",
    "Instead of aligning the body detected for each image, we anchored the position of the detected nose and calculated the distance between each keypoint to the nose coordinate. This avoids aligning the body detected in each image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbd6600",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "Below are some example images of how the human pose detection works on our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668fd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrat Pose using MoveNet\n",
    "%%time\n",
    "def extract_pose():\n",
    "    original_backend = matplotlib.get_backend()\n",
    "    print(f'Switching MatPlotLib backend from {original_backend} to Agg')\n",
    "    # Pose extraction uses plt's canvas. We need a non-interactive backend to avoid memory leaks.\n",
    "    matplotlib.use('Agg')\n",
    "    pose_extractor = eda_helpers.PoseExtractor(config, face_config, pose_config, tqdm)\n",
    "    pose_extractor.extract_poses(pose_config.FEATURES_FOLDER, pose_config.SUMMARY_NAME)\n",
    "    matplotlib.use(original_backend)\n",
    "\n",
    "# extract_pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29ff3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of extracted pose\n",
    "%matplotlib inline\n",
    "# reset matplotlib's backend in case the previous cell left the Agg backend active.\n",
    "vizualizer.display_poses(rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f26713f",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "We can see pretty apparent differences in the distance from the nose distributions across the different classes which make this feature a good candidate for our classifier. For example, right wrist y-coordinates are much lower in the reaching back class when compared to the rest. Another example is that for drinking and phone calling, the right wrist height is mostly aligned with the nose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e554dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of keypoint features distance\n",
    "vizualizer.plot_keypoints_relative_positions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1234a235",
   "metadata": {},
   "source": [
    "### Arm Angle Feature\n",
    "#### Intuition\n",
    "Again here we are taking advantage of the relatively stable body parts detection due the fixed camera angle in the image. The arm angle of the driver could help us detect distracted drivers, for example distinguishing a driver who is having his/her both hands on the wheel and the ones who are using the right hand to hold a phone, a beverage, playing with console or reaching to get something from a seat. Our intuition is that safe driving can be place in the former category (both hands on the steering wheel) and unsafe (one arm is off). \n",
    "Our goal in this feature extraction exercise is to detect location of lower right and left arms and measure their angles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c28e64",
   "metadata": {},
   "source": [
    "#### Data Preprocessing\n",
    "We first use Tensorflow’s pretrained body segmentation model “bodypix” to identify and segment out the left and right lower arms’ coordinates in the training data. The bodypix package color-codes different body parts. By knowing colors of lower right and left arms, we calculate their colored areas. We then estimated the center of the arms to find the arms’ location, fitted a straight line through the arms’ center and calculated the angle of the arms by fitting a line using the least squares polynomial fit package (numpy.polyfit) through the area.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01841ab9",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "Arms are identified and angles are calculated below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60efafd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization on detected arms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1970b3",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "Using the box plot we can see distinguishable difference in angle and dispersion between class 0 (safe driving) and most of the other classes, especially for the right arm, which makes us believe that it is a useful feature. \n",
    "Angles of the left arm are less telling but in combination with the right are should generate good results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3119f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of arms\n",
    "\n",
    "# Get box plots information for left arm-orientation\n",
    "df_without_na = df.dropna()\n",
    "class_tags = df['Class'].unique()\n",
    "list_left_arm_orients = list()\n",
    "for this_tag in class_tags:\n",
    "    left_arm_orients = df_without_na.loc[df['Class'] == this_tag]['LeftOrient'].tolist()\n",
    "    list_left_arm_orients.append(left_arm_orients)\n",
    "# Plot box-plot for right arm\n",
    "ax = sns.boxplot(list_left_arm_orients)\n",
    "ax.set_xticklabels(class_tags.tolist())\n",
    "ax.set(xlabel='Class tags', ylabel='Orientation (in rad)', title='Left arm orientations across different classes')\n",
    "plt.show()\n",
    "\n",
    "# Get box plots information for right arm-orientation\n",
    "df_without_na = df.dropna()\n",
    "class_tags = df['Class'].unique()\n",
    "list_right_arm_orients = list()\n",
    "for this_tag in class_tags:\n",
    "    right_arm_orients = df_without_na.loc[df['Class'] == this_tag]['RightOrient'].tolist()\n",
    "    list_right_arm_orients.append(right_arm_orients)\n",
    "# Plot box-plot for right arm\n",
    "ax = sns.boxplot(list_right_arm_orients)\n",
    "ax.set_xticklabels(class_tags.tolist())\n",
    "ax.set(xlabel='Class tags', ylabel='Orientation (in rad)', title='Right arm orientations across different classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42036057",
   "metadata": {},
   "source": [
    "#### Other Trials and Errors\n",
    "We have tried other features such as HOG, Pixel, and Canny Edge Detection. These features require alignment of the image which is hard to achieve and therefore perform poorly as we took them as-is.\n",
    "\n",
    "We also tried eyes detection, but it has a low coverages as many of the images have facial objects covering parts or entirity of the eyes (ex. Sunglasses)\n",
    "\n",
    "In addition, we tried using CNN to extract features. With limited computational resources, the performance of this feature is also poor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed88a632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "%%time  \n",
    "# faces, original_images, poses, y, filenames = load_data(30, other_types=[enums.ImageTypes.ORIGINAL, enums.ImageTypes.POSE, enums.ImageTypes.FACE], included_labels=config.included_labels)\n",
    "feature_extractor = feature_helpers.FeatureExtractor(config, face_config, pose_config, tqdm)\n",
    "data = feature_extractor.load_data(image_types=IMAGE_TYPES, sample_type=enums.SampleType.TRAIN_TEST_VALIDATION, shuffle=True)\n",
    "\n",
    "# load existing features and re-generate them only if needed.\n",
    "features_list = feature_extractor.load_feature_vectors(config.FEATURE_VECTORS_FOLDER, data[enums.DataColumn.FILENAME.value], data[enums.DataColumn.LABEL.value])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee8649e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract HOG feature\n",
    "\n",
    "%%time\n",
    "hog_features, hogs = feature_extractor.get_hog_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ba56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pixel feature\n",
    "\n",
    "%%time\n",
    "pixel_features = feature_extractor.get_pixel_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9f7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract CNN feature\n",
    "%%time\n",
    "# GPU does not seems to help much.\n",
    "cnn_features = feature_extractor.get_cnn_features(data[enums.ImageTypes.ORIGINAL.name.lower()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b3d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Canny Edges\n",
    "\n",
    "%%time\n",
    "canny_features, cannies = feature_extractor.get_canny_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1461e855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract pose\n",
    "\n",
    "%%time\n",
    "pose_features = feature_extractor.get_pixel_features(data[enums.ImageTypes.POSE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a5be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract number of eyes detected feature\n",
    "eye_count = feature_extractor.detect_eyes(config.TRAIN_DATA, data[enums.DataColumn.LABEL.name.lower()], data[enums.DataColumn.FILENAME.name.lower()])\n",
    "\n",
    "def eye_summary(eye_count):\n",
    "    df = pd.DataFrame(eye_count, columns=['count'])\n",
    "    print(f'0: {df[df[\"count\"] == 0].shape[0]}, 1: {df[df[\"count\"] == 1].shape[0]}, 2: {df[df[\"count\"] == 2].shape[0]}')\n",
    "          \n",
    "eye_summary(eye_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the generated feature vectors.\n",
    "features_list = [pixel_features, hog_features, cnn_features, canny_features, pose_features, body_parts_features]\n",
    "feature_extractor.save_feature_vectors(config.FEATURE_VECTORS_FOLDER, data['filename'], data['label'], features_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c977aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA on features\n",
    "\n",
    "print(f'Loaded {data.shape[0]} samples.')\n",
    "print(f'hog_features:{hog_features.shape}, hog_features.min:{np.min(hog_features)}, hog_features.max:{np.max(hog_features)}')\n",
    "print(f'pixel_features:{pixel_features.shape}, pixel_features.min:{np.min(pixel_features)}, pixel_features.max:{np.max(pixel_features)}')\n",
    "print(f'cnn_features:{cnn_features.shape}, cnn_features.min:{np.min(cnn_features)}, cnn_features.max:{np.max(cnn_features)}')\n",
    "print(f'canny_features:{canny_features.shape}, canny_features.min:{np.min(canny_features)}, canny_features.max:{np.max(canny_features)}')\n",
    "print(f'pose_features:{pose_features.shape}, pose_features.min:{np.min(pose_features)}, pose_features.max:{np.max(pose_features)}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2364614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize features\n",
    "vizualizer.plot_features(included_labels=config.class_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e642a72",
   "metadata": {},
   "source": [
    "## Classification\n",
    "### Dimensionality Reduction\n",
    "We tried two different methods of dimensionality reduction. Both methods look fine with clear clusters of classes visible after the reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcd249",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers for vizualizing\n",
    "def plot_PCA(X_list, names, n_components, max_components, out_file='pca.jpg', ):\n",
    "    pca_list, xpca_list = feature_extractor.get_PCA(X_list, n_components=n_components)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    colors = ['b-', 'g-', 'r-', 'k-', 'y-']\n",
    "    plot_labels = [f'{name} features' for name in names]\n",
    "    for i in range(len(X_list)):\n",
    "        plt.plot(np.cumsum(pca_list[i].explained_variance_ratio_), colors[i], label=plot_labels[i])\n",
    "    # plt.xticks(np.arange(max_components)+1)\n",
    "    plt.yticks(np.linspace(0, 1, 8))\n",
    "    plt.grid(visible=True)\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Explained Variances')\n",
    "    plt.legend()\n",
    "    plt.title('Explaining Power Of Principal Components')\n",
    "    plt.tight_layout(pad=0.1, h_pad=None, w_pad=None, rect=None)\n",
    "    plt.savefig(f'{config.OUTPUT_FOLDER}/report_plots/{out_file}', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "def plot_classes(X, y, ax, title, included_labels):\n",
    "    colormap = plt.cm.gist_rainbow # hsv tab20 #nipy_spectral #, Set1,Paired\n",
    "    colorst = [colormap(i) for i in np.linspace(0, 1.0, len(np.unique(y)))]\n",
    "    markers = ['o', 'v', 's', 'p', 'x', '>', '*', '<', 'P', '^']\n",
    "    for k, label in enumerate(included_labels):\n",
    "        marker = markers[k % len(markers)]\n",
    "        if X.shape[1] == 2:\n",
    "            ax.scatter(X[y==label, 0], X[y==label, 1], facecolors=colorst[k], marker=marker, label=config.class_dict[label])\n",
    "        else:\n",
    "            ax.scatter(X[y==label, 0], X[y==label, 1], X[y==label, 2], facecolors=colorst[k], marker=marker, label=config.class_dict[label])\n",
    "    ax.set_title(title)\n",
    "    \n",
    "def plot_components(features_list, X_pcas, X_tsnes, names, included_labels=LABELS_TO_INCLUDE, out_file='clustering.jpg'):\n",
    "    # project the features into 2 dimensions\n",
    "    fig, ax = plt.subplots(nrows=len(features_list), ncols=2, figsize=(10,5))\n",
    "    if len(features_list) == 1:\n",
    "        ax = [ax]\n",
    "\n",
    "    # y is the class labels\n",
    "    for i in range(len(features_list)):\n",
    "        plot_classes(X_pcas[i], y, ax[i][0], title=f'{names[i]} PCA', included_labels=LABELS_TO_INCLUDE)\n",
    "        plot_classes(X_tsnes[i], y, ax[i][1], title=f'{names[i]} tSNE', included_labels=LABELS_TO_INCLUDE)\n",
    "    \n",
    "    handles, plot_labels = ax[0][0].get_legend_handles_labels()\n",
    "    fig.legend(handles, plot_labels, loc='upper center')\n",
    "    plt.tight_layout(pad=0.1, h_pad=None, w_pad=12, rect=None)\n",
    "    plt.savefig(f'{config.OUTPUT_FOLDER}/report_plots/{out_file}', dpi=300)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae147a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA\n",
    "%%time\n",
    "plot_PCA([cnn_features, keypoints_features], ['CNN', 'Keypoints'], n_components=[200, 26], max_components=200, out_file=f'pca.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e649581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize components\n",
    "%%time\n",
    "def visualize_components():\n",
    "    features_list = [cnn_features, keypoints_features]\n",
    "    n_components = [2, 2]\n",
    "    names = ['CNN', 'Keypoints']\n",
    "    pcas = feature_extractor.get_PCA(features_list, n_components)[-1]\n",
    "    tsnes = feature_extractor.get_tsne(features_list, n_components=2)\n",
    "    plot_components(features_list, pcas, tsnes, names, included_labels=LABELS_TO_INCLUDE)\n",
    "visualize_components()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d2fadc",
   "metadata": {},
   "source": [
    "### Hyperparameter Search\n",
    "#### Setup\n",
    "#### Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1885b479",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "#### Justification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386846c7",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f30597",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "#### Justification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469895f0",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad47ea8a",
   "metadata": {},
   "source": [
    "## Lesson Learned and Future Improvement\n",
    "### Data\n",
    "Noise in the data affects the quality of our features. For example, eyes are hard to detect for the driver who is wearing sunglasses. Another example is that the faces we see in the training data are oftentimes not front facing, which also challenged a facial keypoint detection.\n",
    "\n",
    "### Classifiers\n",
    "Logistic regression assumes independence and our features are not perfectly independent. For example, the relative position features could be correlated with the arm angle feature.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
