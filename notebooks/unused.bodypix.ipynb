{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e05ecb6b-6bc4-402a-8883-3b0861313f45",
   "metadata": {},
   "source": [
    "# Human Body Segmentation\n",
    "This notebook uses Python version of Google's [BodyPix API](https://blog.tensorflow.org/2019/11/updated-bodypix-2.html) to identify face, arms and torso of drivers in each image of our dataset. Idea is to first isolate the body segments and then compute local features for training.\n",
    "\n",
    "Reference inference implementation: \n",
    "* https://www.kaggle.com/code/rkuo2000/bodypix\n",
    "* https://www.kaggle.com/code/rkuo2000/bodypix\n",
    "\n",
    "## Prerequesites\n",
    "Install the bodypix library using\n",
    "```\n",
    "!pip install tf-bodypix\n",
    "!pip install tfjs-graph-converter\n",
    "```\n",
    "\n",
    "## TODO\n",
    "* Establish a pipeline that processes all the training data (and test data for now) to create the segmentation map or bounding box as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17bb2af-6d5f-4e55-a4ae-bfce161a7dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cbe4e25-42ca-4396-85df-f4c6513639e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "from tf_bodypix.api import load_model, download_model, BodyPixModelPaths\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import statistics\n",
    "import preprocessor\n",
    "import configuration\n",
    "import viz\n",
    "\n",
    "preproc = preprocessor.PreProcessor()\n",
    "config = configuration.Configuration()\n",
    "vizualizer = viz.Vizualizer()\n",
    "\n",
    "PART_CHANNELS = [\n",
    "    'left_face',\n",
    "    'right_face',\n",
    "    'left_upper_arm_front',\n",
    "    'left_upper_arm_back',\n",
    "    'right_upper_arm_front',\n",
    "    'right_upper_arm_back',\n",
    "    'left_lower_arm_front',\n",
    "    'left_lower_arm_back',\n",
    "    'right_lower_arm_front',\n",
    "    'right_lower_arm_back',\n",
    "    'left_hand',\n",
    "    'right_hand',\n",
    "    'torso_front',\n",
    "    'torso_back',\n",
    "    'left_upper_leg_front',\n",
    "    'left_upper_leg_back',\n",
    "    'right_upper_leg_front',\n",
    "    'right_upper_leg_back',\n",
    "    'left_lower_leg_front',\n",
    "    'left_lower_leg_back',\n",
    "    'right_lower_leg_front',\n",
    "    'right_lower_leg_back',\n",
    "    'left_feet',\n",
    "    'right_feet'\n",
    "]\n",
    "\n",
    "parts_of_interest = [\n",
    "    'left_face',\n",
    "    'right_face',\n",
    "    'left_upper_arm_front',\n",
    "    'left_upper_arm_back',\n",
    "    'right_upper_arm_front',\n",
    "    'right_upper_arm_back',\n",
    "    'left_lower_arm_front',\n",
    "    'left_lower_arm_back',\n",
    "    'right_lower_arm_front',\n",
    "    'right_lower_arm_back',\n",
    "    'left_hand',\n",
    "    'right_hand',\n",
    "    'torso_front',\n",
    "    'torso_back'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa858759-2ee9-4dd0-bb7a-ef1538bb4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "        # Available models:\n",
    "    # MOBILENET_FLOAT_50_STRIDE_8\n",
    "    # MOBILENET_FLOAT_50_STRIDE_16\n",
    "    # MOBILENET_FLOAT_75_STRIDE_8\n",
    "    # MOBILENET_FLOAT_75_STRIDE_16\n",
    "    # MOBILENET_FLOAT_100_STRIDE_8\n",
    "    # MOBILENET_FLOAT_100_STRIDE_16\n",
    "    # RESNET50_FLOAT_STRIDE_16\n",
    "    # RESNET50_FLOAT_STRIDE_32\n",
    "\n",
    "    # bp_model = load_model(download_model(BodyPixModelPaths.MOBILENET_FLOAT_50_STRIDE_16))\n",
    "    # MOBILENET_FLOAT_75_STRIDE_8\n",
    "    # MOBILENET_FLOAT_100_STRIDE_8\n",
    "    # MOBILENET_FLOAT_75_STRIDE_16\n",
    "    # MOBILENET_FLOAT_100_STRIDE_16\n",
    "    # bp_model = load_model(download_model(BodyPixModelPaths.MOBILENET_FLOAT_100_STRIDE_16))\n",
    "    bp_model = load_model(download_model(BodyPixModelPaths.RESNET50_FLOAT_STRIDE_16))\n",
    "    return bp_model\n",
    "\n",
    "    \n",
    "def infer(ax, bp_model, image_file, parts=True):\n",
    "    preparer = preprocessor.PreProcessor()\n",
    "    image = preparer.increase_brightness(cv2.imread(image_file), 50)\n",
    "    \n",
    "    start = time.process_time()\n",
    "    prediction = bp_model.predict_single(image) # Passing the image to the model\n",
    "    time_taken = time.process_time() - start\n",
    "    mask = prediction.get_mask(threshold=0.2).numpy().astype(np.uint8)\n",
    "    part_masks = prediction.get_part_mask(mask, part_names=parts_of_interest)\n",
    "    # colored_mask = prediction.get_colored_part_mask(mask)\n",
    "    colored_mask = prediction.get_colored_part_mask(mask, part_names=parts_of_interest)\n",
    "    # new_mask = cv2.bitwise_and(image, image, mask=mask)\n",
    "    masked = cv2.addWeighted(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 0.5, colored_mask, 0.5, 0, dtype = cv2.CV_8U)\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    bounding_boxes = [cv2.boundingRect(contour) for contour in contours]\n",
    "    if len(bounding_boxes)>0:\n",
    "        for x,y,w,h in bounding_boxes:\n",
    "            rect = patches.Rectangle((x,y), w, h, linewidth=1, edgecolor='r', facecolor='none')\n",
    "            ax.add_patch(rect)\n",
    "    ax.imshow(masked)\n",
    "    # ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    return time_taken\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39df240-a1d9-440e-86cb-f4b709227a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# infer(\"/Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/input/state-farm-distracted-driver-detection/imgs/test_10/img_24.jpg\")\n",
    "bp_model = get_model()\n",
    "for cur_class in range(len(config.class_dict)):\n",
    "    times = []\n",
    "    times = [*times, *vizualizer.infer_and_plot(bp_model, infer, config.class_dict[cur_class], f'{config.IMAGES_BASE}/c{cur_class}', num_images=20, cols=4, plt_width=10, plt_height=10, out_file=f'{config.OUTPUT_FOLDER}/body_segmentation_class_{cur_class}.png')]\n",
    "\n",
    "print(f'Average time for inference:{statistics.mean(times)} seconds')\n",
    "print(f'Anticipated preprocessing time for training dataset of 22,000:{22000 * statistics.mean(times) / 60} mins')\n",
    "\n",
    "# Since output images a large and are saved a JPGs, we erase the output cell.\n",
    "clear_output(wait=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca63990-b079-459c-9ec8-81fdeba6849d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
