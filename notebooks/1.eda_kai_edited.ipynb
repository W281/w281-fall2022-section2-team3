{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4669f297-d659-453d-a2e2-551c36fe10bf",
   "metadata": {},
   "source": [
    "# EDA and Data Preparation\n",
    "This notebook uses [Pytorch's MTCNN](https://pypi.org/project/facenet-pytorch/) face detector to extract the faces from the images and [Movenet model from TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/movenet) for extracting poses.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "* Install the pytorch's facenet library using `!pip install facenet-pytorch`\n",
    "* Download [lightning](https://tfhub.dev/google/movenet/singlepose/lightning/4) version of the movenet model from the TF hub.\n",
    "* Good to enable GPU as the inference is a little faster on it.\n",
    "## Step 0: Notebook Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee50a6e-f278-422d-ba2d-c844ab37a4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e00710-fca8-41ce-8319-56209c1d1503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib\n",
    "import configuration\n",
    "import customdataset\n",
    "import enums\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import eda_helpers\n",
    "from tqdm.notebook import tqdm\n",
    "# import transformers\n",
    "from torch.utils.data import DataLoader\n",
    "import seaborn as sns\n",
    "from torchvision import transforms\n",
    "import transformers\n",
    "import feature_helpers\n",
    "\n",
    "device = 'cpu'\n",
    "config = configuration.Configuration()\n",
    "face_config = configuration.FaceConfig(config)\n",
    "pose_config = configuration.PoseConfig(config)\n",
    "\n",
    "IMAGE_TYPES = [enums.ImageTypes.ORIGINAL, enums.ImageTypes.POSE, enums.ImageTypes.FACE]\n",
    "feature_extractor = feature_helpers.FeatureExtractor(config, face_config, pose_config, tqdm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d9467-8ba4-4d70-afdf-57164d57b2d3",
   "metadata": {},
   "source": [
    "## Step 1: Understanding The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92887ad-c2b0-447e-a8ac-ddc56e162446",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_counts():    \n",
    "    classes = list(config.class_dict.keys())\n",
    "    classes.sort()\n",
    "    d = {\"img\" : [], \"class\" : []}\n",
    "    for c in classes:\n",
    "        imgs = [img for img in os.listdir(os.path.join(config.TRAIN_DATA, f'c{c}')) if not img.startswith(\".\")]\n",
    "        for img in imgs:\n",
    "            d[\"img\"].append(img)\n",
    "            d[\"class\"].append(config.class_dict.get(c))\n",
    "    df = pd.DataFrame(d)\n",
    "    ax = sns.countplot(data=df, y=\"class\", palette='Set2')\n",
    "    ax.set(title=\"Distribution of Training Data\")\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_ylabel('Class')\n",
    "    # ax.tick_params(axis='x', rotation=90)\n",
    "    plt.savefig(f'{config.OUTPUT_FOLDER}/class_distribution.png', dpi=300, bbox_inches = \"tight\")\n",
    "    plt.tight_layout()\n",
    "    # print(\"Total Number Of Images In Training Data :\",len(df))\n",
    "\n",
    "plot_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa9824-a8db-4dd3-a20a-4394eb978997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 10 images per category, with category name.\n",
    "# TODO: Refactor and use dataloader for random sampling\n",
    "def sample(c, num):\n",
    "    # TODO: Do random sampling instead of the first num images.\n",
    "    d = [os.path.join(config.TRAIN_DATA, f'c{c}', img) for img in os.listdir(os.path.join(config.TRAIN_DATA, f'c{c}')) if not img.startswith(\".\")]\n",
    "    return d[0:num], config.class_dict.get(c)\n",
    "\n",
    "def plot_samples(num=5):\n",
    "    # load images per class\n",
    "    # Display them in each row.\n",
    "    fig, axes = plt.subplots(10, num, figsize=[21, 42])\n",
    "    pad = 5 # in points\n",
    "    classes = list(config.class_dict.keys())\n",
    "\n",
    "    for row, class_code in enumerate(classes):\n",
    "        imgs, class_name = sample(class_code, num)      \n",
    "        for i, img_file in enumerate(imgs):\n",
    "            img = plt.imread(img_file)\n",
    "            axis = axes[row][i]\n",
    "            if i == 0:\n",
    "                axis.annotate(class_name, xy=(0.5, 1), xytext=(0, pad),\n",
    "                    xycoords='axes fraction', textcoords='offset points',\n",
    "                    fontsize=24, ha='center', va='baseline')\n",
    "            axis.imshow(img)\n",
    "            axis.set_xticks([])\n",
    "            axis.set_yticks([])\n",
    "\n",
    "    #   fig.suptitle('Samples From Each Class', fontsize=32, y=0.99)\n",
    "    plt.subplots_adjust(wspace=None, hspace=None)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.88)\n",
    "    plt.savefig(f'{config.OUTPUT_FOLDER}/class_samples.png', dpi=300, bbox_inches = \"tight\")\n",
    "    plt.show()\n",
    "      \n",
    "plot_samples(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48b4486-2111-41ce-ae7f-4afa7f3d9ab1",
   "metadata": {},
   "source": [
    "## Step 2: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f84fa-b7fb-4099-9f44-a608e89daeb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Extract face from each image. Takes a few hours to run.\n",
    "def extract_faces():\n",
    "    face_extractor = eda_helpers.FaceExtractor(config, tqdm)\n",
    "    face_extractor.extract_faces(face_config.FEATURES_FOLDER, face_config.FACE_SUMMARY_NAME, config.ANNOTATION_FILE)\n",
    "    # face_extractor.extract_faces(f'{config.OUTPUT_FOLDER}/tt', face_config.FACE_SUMMARY_NAME, f'{config.OUTPUT_FOLDER}/tt/annotated.csv', limit=5)\n",
    "# extract_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9482c1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kai added for eye detection\n",
    "# package required: cv2, numpy\n",
    "# file required: haarcascade_eye.xml\n",
    "\n",
    "# create a list to store number of eyes detected\n",
    "n_eyes_detected = []\n",
    "\n",
    "for file in files: # need to modify this list of files to the location of the training data\n",
    "    # turn the image to gray image\n",
    "    image = cv2.imread(file)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    eye_cascade = cv2.CascadeClassifier('./input/haarcascade_eye.xml') # need to save this xml to your folder and change the path\n",
    "    eyes = eye_cascade.detectMultiScale(gray, scaleFactor = 1.3, minNeighbors = 5)\n",
    "    \n",
    "    # save the number of eyes detcted\n",
    "    n_eyes_detected.append(len(eyes))\n",
    "    \n",
    "    # display eye detection image\n",
    "    for (x,y,w,h) in eyes:\n",
    "            cv2.rectangle(image,(x,y),(x+w,y+h),(0, 255, 0),5)\n",
    "    \n",
    "    plt.imshow(image)\n",
    "#     # or you could use cv2.imshow to display\n",
    "#     cv2.imshow(\"Eyes Detected\", image)\n",
    "#     cv2.waitKey(0)\n",
    "\n",
    "# read the annotation file\n",
    "df = pd.read_csv('annotation_file.csv') # need to modify this path to the correct path to the annotation_file\n",
    "\n",
    "# append the eye feature column\n",
    "df[num_of_eyes_detected] = n_eyes_detected\n",
    "\n",
    "# save the csv file again\n",
    "df.to_csv('annotation_file.csv') # need to modify this path to the correct path to the annotation_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6651f79c-e336-4f72-ae3c-6c4ac1251f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(summary_csv):\n",
    "    df_summary = pd.read_csv(summary_csv)\n",
    "    df_summary['class_name'] = df_summary['class'].map(config.class_dict)\n",
    "    \n",
    "    df_0 = df_summary[df_summary['num_faces'] == 0].groupby('class_name').agg(\n",
    "        no_faces=pd.NamedAgg(column=\"filename\", aggfunc=\"count\")\n",
    "    )\n",
    "    df_1 = df_summary[df_summary['num_faces'] == 1].groupby('class_name').agg(\n",
    "        one_face=pd.NamedAgg(column=\"filename\", aggfunc=\"count\")\n",
    "    )\n",
    "    df_gt_1 = df_summary[df_summary['num_faces'] > 0].groupby('class_name').agg(\n",
    "        many_faces=pd.NamedAgg(column=\"filename\", aggfunc=\"count\")\n",
    "    )\n",
    "    \n",
    "    pd_merged = pd.merge(pd.merge(df_0, df_1, on='class_name'), df_gt_1, on='class_name')\n",
    "    pd_merged.plot(title='Face Identification Summary', figsize=(8, 6), kind='bar', \n",
    "                   ylabel='', xlabel='', rot=45, grid=False)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "summary(f'{face_config.FEATURES_FOLDER}/{face_config.FACE_SUMMARY_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa9080c-45e8-42c6-a135-1bea50592f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the results of face detection.\n",
    "def display_faces(per_group_count=3):\n",
    "    def get_images(sample_type):\n",
    "        dataset = customdataset.MainDataset(config, face_config, pose_config,\n",
    "                                            sample_type=sample_type,\n",
    "                                            image_types=[enums.ImageTypes.FACE_ANNOTATED])\n",
    "        \n",
    "        dataloader = DataLoader(dataset, num_workers=0, batch_size=1,\n",
    "                                shuffle=True, collate_fn=dataset.get_image_from)\n",
    "        sampled = []\n",
    "        for i, sample in enumerate(dataloader):\n",
    "            if i >= per_group_count:\n",
    "                break\n",
    "            images, label, filename = sample\n",
    "            image = images[enums.ImageTypes.FACE_ANNOTATED]\n",
    "            sampled.append(image)\n",
    "        return sampled\n",
    "    \n",
    "    fig, axes = plt.subplots(per_group_count, 3, figsize=[18, 15], dpi=72)\n",
    "    sampled = list(zip(get_images(enums.SampleType.WITH_NO_FACE),\n",
    "                       get_images(enums.SampleType.WITH_JUST_ONE_FACE), \n",
    "                       get_images(enums.SampleType.WITH_MORE_THAN_ONE_FACE)))\n",
    "    titles = ['No Face Identified', 'Exactly One Face Identified', 'Multiple Faces Identified']\n",
    "    for i in range(per_group_count):\n",
    "        for j, image in enumerate(sampled[i]):\n",
    "            if i == 0:\n",
    "                axes[i][j].set_title(titles[j], fontsize = 20)\n",
    "            axes[i][j].imshow(image)\n",
    "            axes[i][j].axis('off')\n",
    "    plt.suptitle('Random Samples Annotated With Faces Identified', fontsize = 26)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "display_faces(per_group_count = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3dbf4d-09e7-4daa-8313-9992836cd11b",
   "metadata": {},
   "source": [
    "## Step 3: Train-Test-Validation Split\n",
    "1. Face detection and extraction using MTCNN\n",
    "2. Take 600 images with at least one face identified in each class\n",
    "3. Create a 80-10-10 split to represent the train-validation-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632eb5f-4153-4ba3-ab34-ccaaba0b7b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset():\n",
    "    splitter = eda_helpers.SampleSplitter(config, face_config, pose_config, tqdm=tqdm)\n",
    "    splitter.sample(config.class_dict.keys(), samples_per_class=[600, 480, 60, 60], out_file=config.ANNOTATION_FILE)\n",
    "\n",
    "# split_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aa72fd-5ee1-44cd-97bb-1c7a9cacca54",
   "metadata": {},
   "source": [
    "## Step 4: Pose Detection For Train-Test-Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acab85b-4c59-4c0f-8b4f-57d4c68aad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pose_config = configuration.PoseConfig(config)\n",
    "def extract_pose():\n",
    "    original_backend = matplotlib.get_backend()\n",
    "    print(f'Switching MatPlotLib backend from {original_backend} to Agg')\n",
    "    # Pose extraction uses plt's canvas. We need a non-interactive backend to avoid memory leaks.\n",
    "    matplotlib.use('Agg')\n",
    "    \n",
    "    pose_extractor = eda_helpers.PoseExtractor(config, face_config, pose_config, tqdm)\n",
    "    pose_extractor.extract_poses(pose_config.FEATURES_FOLDER, pose_config.SUMMARY_NAME)\n",
    "    # pose_extractor.extract_poses(f'{config.OUTPUT_FOLDER}/tt', pose_config.SUMMARY_NAME)\n",
    "    matplotlib.use(original_backend)\n",
    "\n",
    "# extract_pose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5787d-44df-48c3-b5d9-4ed5ad6c1281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_poses(rows=3):\n",
    "    dataset = customdataset.MainDataset(config, face_config, pose_config,\n",
    "                                        sample_type=enums.SampleType.TRAIN_VALIDATION,\n",
    "                                        image_types=[enums.ImageTypes.ORIGINAL, enums.ImageTypes.POSE_ANNOTATED, enums.ImageTypes.POSE])\n",
    "\n",
    "    dataloader = DataLoader(dataset, num_workers=0, batch_size=1,\n",
    "                            shuffle=True, collate_fn=dataset.get_image_from)\n",
    "    fig, axes = plt.subplots(rows, 3, figsize=[18, 15], dpi=72)\n",
    "    axes[0][0].set_title('Original', fontsize = 20)\n",
    "    axes[0][1].set_title('Annotated With Pose', fontsize = 20)\n",
    "    axes[0][2].set_title('Pose Extracted', fontsize = 20)\n",
    "\n",
    "    for i, sample in enumerate(dataloader):\n",
    "        if i >= rows:\n",
    "            break\n",
    "        images, label, filename = sample\n",
    "        axes[i][0].imshow(images[enums.ImageTypes.ORIGINAL])\n",
    "        axes[i][0].axis('off')\n",
    "        axes[i][1].imshow(images[enums.ImageTypes.POSE_ANNOTATED])\n",
    "        axes[i][1].axis('off')\n",
    "        axes[i][2].imshow(images[enums.ImageTypes.POSE])\n",
    "        axes[i][2].axis('off')\n",
    "    plt.suptitle('Random Samples Annotated With Poses', fontsize = 26)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "display_poses(rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e279c55-322d-421c-a813-70b03990e249",
   "metadata": {},
   "source": [
    "## Step 5: Extract Feature Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992981a0-f705-4247-8999-7f41a94e0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time  \n",
    "# faces, original_images, poses, y, filenames = load_data(30, other_types=[enums.ImageTypes.ORIGINAL, enums.ImageTypes.POSE, enums.ImageTypes.FACE], included_labels=config.included_labels)\n",
    "feature_extractor = feature_helpers.FeatureExtractor(config, face_config, pose_config, tqdm)\n",
    "data = feature_extractor.load_data(image_types=IMAGE_TYPES, sample_type=enums.SampleType.TRAIN_TEST_VALIDATION, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb75a32-b4bd-4eaa-b433-efe266fde75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "hog_features, hogs = feature_extractor.get_hog_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab2d5f-9ab0-401f-a589-48d622240663",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pixel_features = feature_extractor.get_pixel_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca0238-ff91-487b-af89-069bd685b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GPU does not seemt to help much.\n",
    "cnn_features = feature_extractor.get_cnn_features(data[enums.ImageTypes.ORIGINAL.name.lower()], device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a283df-7828-40c2-841b-78823a7efe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "canny_features, cannies = feature_extractor.get_canny_features(data[enums.ImageTypes.FACE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d4777-94f7-430f-988e-a1756b9998c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pose_features = feature_extractor.get_pixel_features(data[enums.ImageTypes.POSE.name.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf12b8-f242-4094-9314-22b14e831372",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Save the generated feature vectors.\n",
    "features_list = [pixel_features, hog_features, cnn_features, canny_features, pose_features]\n",
    "feature_extractor.save_feature_vectors(config.FEATURE_VECTORS_FOLDER, data['filename'], data['label'], features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbef6e-2bd8-491e-ab7c-9f0c5ecb2001",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loaded {data.shape[0]} samples.')\n",
    "print(f'hog_features:{hog_features.shape}, hog_features.min:{np.min(hog_features)}, hog_features.max:{np.max(hog_features)}')\n",
    "print(f'pixel_features:{pixel_features.shape}, pixel_features.min:{np.min(pixel_features)}, pixel_features.max:{np.max(pixel_features)}')\n",
    "print(f'cnn_features:{cnn_features.shape}, cnn_features.min:{np.min(cnn_features)}, cnn_features.max:{np.max(cnn_features)}')\n",
    "print(f'canny_features:{canny_features.shape}, canny_features.min:{np.min(canny_features)}, canny_features.max:{np.max(canny_features)}')\n",
    "print(f'pose_features:{pose_features.shape}, pose_features.min:{np.min(pose_features)}, pose_features.max:{np.max(pose_features)}')\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63765aff-2712-4077-818b-62cc2f77d121",
   "metadata": {},
   "source": [
    "## Step 6. Visualize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3dc827-7674-4f87-bb02-a3d02c83056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(included_labels):\n",
    "    def display_img(ax_idx, img, row_name, col_name, first_row, first_col):\n",
    "        ax = axes[ax_idx]\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        if first_row:\n",
    "            ax.set_title(col_name, fontsize=32)\n",
    "        if first_col:\n",
    "            ax.set_ylabel(row_name, fontsize = 32)\n",
    "        ax.xaxis.set_major_locator(ticker.NullLocator())\n",
    "        ax.yaxis.set_major_locator(ticker.NullLocator())\n",
    "        \n",
    "    # Load the data\n",
    "    image_types = [\n",
    "        enums.ImageTypes.FACE_ANNOTATED,\n",
    "        enums.ImageTypes.POSE_ANNOTATED,\n",
    "        enums.ImageTypes.ORIGINAL,\n",
    "        enums.ImageTypes.POSE, \n",
    "        enums.ImageTypes.FACE\n",
    "    ]\n",
    "    count_per_label = 1\n",
    "    data = feature_extractor.load_data(image_types=image_types, shuffle=True, \n",
    "                                       sample_type=enums.SampleType.TRAIN_VALIDATION,\n",
    "                                       count_per_label=count_per_label, include_feature_vectors=True)\n",
    "    _, hogs = feature_extractor.get_hog_features(data[enums.DataColumn.FACE.value])\n",
    "    data['hog_img'] = hogs\n",
    "    _, cannies = feature_extractor.get_canny_features(data[enums.DataColumn.FACE.value])\n",
    "    data['canny_img'] = cannies\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=len(included_labels), ncols=len(image_types) + 2, figsize=[36, 48], dpi=72)\n",
    "    axes = axes.flatten()\n",
    "    first_row = True\n",
    "    with tqdm(unit='images', desc=f'Loading image', total=len(included_labels)*len(image_types) + 2) as pbar:\n",
    "        for i, cur_label in enumerate(included_labels):\n",
    "            label_df = data[data['label'] == cur_label]\n",
    "            for index, row in label_df.iterrows():\n",
    "                cur_filename = row[enums.DataColumn.FILENAME.value]\n",
    "                face_annotated_img = row[enums.DataColumn.FACE_ANNOTATED.value]\n",
    "                pose_annotated_img = row[enums.DataColumn.POSE_ANNOTATED.value]\n",
    "                original_img = row[enums.DataColumn.ORIGINAL.value]\n",
    "                pose_img = row[enums.DataColumn.POSE.value]\n",
    "                face_img = row[enums.DataColumn.FACE.value]\n",
    "                hog = row['hog_img']\n",
    "                canny = row['canny_img']\n",
    "\n",
    "                imgs_to_display = [\n",
    "                    ('Original', original_img),\n",
    "                    ('Face Annotated', face_annotated_img),\n",
    "                    ('Face Extracted', face_img),\n",
    "                    ('Face - Canny', canny),\n",
    "                    ('Face - Hog', hog),\n",
    "                    ('Pose Annotated', pose_annotated_img),\n",
    "                    ('Pose Extracted', pose_img)\n",
    "                ]\n",
    "                images_per_label = len(imgs_to_display)\n",
    "                first_col = True\n",
    "                for j, item in enumerate(imgs_to_display):\n",
    "                    col_name, img = item\n",
    "                    row_name = config.class_dict[cur_label]\n",
    "                    display_img(i * images_per_label + j, img, row_name, col_name, first_row, first_col)\n",
    "                    pbar.update(1)\n",
    "                    first_col = False\n",
    "            first_row = False\n",
    "\n",
    "        plt.suptitle('Visualizing The Features', fontsize=48)\n",
    "        plt.tight_layout(pad=4, h_pad=None, w_pad=None, rect=None)\n",
    "        plt.show()\n",
    "\n",
    "plot_features(included_labels=config.class_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8fda2-e42f-47d0-b1e2-18c02c6c3904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
