{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c285fae-fc51-4c0c-b383-b2e8fc65c842",
   "metadata": {},
   "source": [
    "# Pose Detection Using Movenet\n",
    "\n",
    "## 1. Introduction\n",
    "This notebook uses [Movenet model from TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/movenet).\n",
    "\n",
    "### 1.1. Pre-requisites\n",
    "* Download [lightning](https://tfhub.dev/google/movenet/singlepose/lightning/4) version of the movenet model from the TF hub.\n",
    "* Good to enable GPU as the inference is a little faster on it.\n",
    "\n",
    "## 2. Extract And Save Poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76fbdd7d-c953-498c-aa9d-077405919e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de73c49-cd0e-4e3d-9fe8-68d3ae76366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import cProfile\n",
    "    \n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "tf.config.list_physical_devices()\n",
    "# tf.debugging.set_log_device_placement(True)\n",
    "# tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "if tf.test.gpu_device_name():\n",
    "    print('GPU found')\n",
    "else:\n",
    "    print(\"No GPU found\")\n",
    "\n",
    "import configuration\n",
    "import customdataset\n",
    "\n",
    "config = configuration.Configuration()\n",
    "face_config = configuration.FaceConfig(config)\n",
    "pose_config = configuration.PoseConfig(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b414cf40-e46b-4cd1-936d-8b32cf9cf62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_gpu_with_cifar():\n",
    "#     cifar = tf.keras.datasets.cifar100\n",
    "#     (x_train, y_train), (x_test, y_test) = cifar.load_data()\n",
    "#     model = tf.keras.applications.ResNet50(\n",
    "#         include_top=True,\n",
    "#         weights=None,\n",
    "#         input_shape=(32, 32, 3),\n",
    "#         classes=100,)\n",
    "\n",
    "#     loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#     model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "#     model.fit(x_train, y_train, epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67f3766e-7b8a-4588-af31-735dd867b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dictionary that maps from joint names to keypoint indices.\n",
    "# KEYPOINT_DICT = {\n",
    "#     'nose': 0,\n",
    "#     'left_eye': 1,\n",
    "#     'right_eye': 2,\n",
    "#     'left_ear': 3,\n",
    "#     'right_ear': 4,\n",
    "#     'left_shoulder': 5,\n",
    "#     'right_shoulder': 6,\n",
    "#     'left_elbow': 7,\n",
    "#     'right_elbow': 8,\n",
    "#     'left_wrist': 9,\n",
    "#     'right_wrist': 10,\n",
    "#     'left_hip': 11,\n",
    "#     'right_hip': 12,\n",
    "#     'left_knee': 13,\n",
    "#     'right_knee': 14,\n",
    "#     'left_ankle': 15,\n",
    "#     'right_ankle': 16\n",
    "# }\n",
    "\n",
    "# # Maps bones to a matplotlib color name.\n",
    "# KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "#     (0, 1): 'm',\n",
    "#     (0, 2): 'c',\n",
    "#     (1, 3): 'm',\n",
    "#     (2, 4): 'c',\n",
    "#     (0, 5): 'm',\n",
    "#     (0, 6): 'c',\n",
    "#     (5, 7): 'm',\n",
    "#     (7, 9): 'm',\n",
    "#     (6, 8): 'c',\n",
    "#     (8, 10): 'c',\n",
    "#     (5, 6): 'y',\n",
    "#     (5, 11): 'm',\n",
    "#     (6, 12): 'c',\n",
    "#     (11, 12): 'y',\n",
    "#     (11, 13): 'm',\n",
    "#     (13, 15): 'm',\n",
    "#     (12, 14): 'c',\n",
    "#     (14, 16): 'c'\n",
    "# }\n",
    "\n",
    "# def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "#                                      height,\n",
    "#                                      width,\n",
    "#                                      keypoint_threshold=0.11):\n",
    "#     \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "#     Args:\n",
    "#         keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "#           the keypoint coordinates and scores returned from the MoveNet model.\n",
    "#     height: height of the image in pixels.\n",
    "#     width: width of the image in pixels.\n",
    "#         keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "#         visualized.\n",
    "\n",
    "#     Returns:\n",
    "#         A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "#           * the coordinates of all keypoints of all detected entities;\n",
    "#           * the coordinates of all skeleton edges of all detected entities;\n",
    "#           * the colors in which the edges should be plotted.\n",
    "#     \"\"\"\n",
    "#     keypoints_all = []\n",
    "#     keypoint_edges_all = []\n",
    "#     edge_colors = []\n",
    "#     num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "#     for idx in range(num_instances):\n",
    "#         kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "#         kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "#         kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "#         kpts_absolute_xy = np.stack(\n",
    "#             [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "#         kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "#             kpts_scores > keypoint_threshold, :]\n",
    "#         keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "#         for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "#             if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "#               kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "#                 x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "#                 y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "#                 x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "#                 y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "#                 line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "#                 keypoint_edges_all.append(line_seg)\n",
    "#                 edge_colors.append(color)\n",
    "#     if keypoints_all:\n",
    "#         keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "#     else:\n",
    "#         keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "#     if keypoint_edges_all:\n",
    "#         edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "#     else:\n",
    "#         edges_xy = np.zeros((0, 2, 2))\n",
    "#     return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "# def draw_prediction_on_image(\n",
    "#     image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "#     output_image_height=None):\n",
    "#     \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "#         Args:\n",
    "#         image: A numpy array with shape [height, width, channel] representing the\n",
    "#           pixel values of the input image.\n",
    "#         keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "#           the keypoint coordinates and scores returned from the MoveNet model.\n",
    "#         crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "#           of the crop region in normalized coordinates (see the init_crop_region\n",
    "#           function below for more detail). If provided, this function will also\n",
    "#           draw the bounding box on the image.\n",
    "#         output_image_height: An integer indicating the height of the output image.\n",
    "#           Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "#       Returns:\n",
    "#         A numpy array with shape [out_height, out_width, channel] representing the\n",
    "#         image overlaid with keypoint predictions.\n",
    "#       \"\"\"\n",
    "#     height, width, channel = image.shape\n",
    "#     aspect_ratio = float(width) / height\n",
    "#     fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "#     # To remove the huge white borders\n",
    "#     fig.tight_layout(pad=0)\n",
    "#     ax.margins(0)\n",
    "#     ax.set_yticklabels([])\n",
    "#     ax.set_xticklabels([])\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     im = ax.imshow(image)\n",
    "#     line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "#     ax.add_collection(line_segments)\n",
    "#     # Turn off tick labels\n",
    "#     scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "#     (keypoint_locs, keypoint_edges,\n",
    "#        edge_colors) = _keypoints_and_edges_for_display(\n",
    "#        keypoints_with_scores, height, width)\n",
    "\n",
    "#     line_segments.set_segments(keypoint_edges)\n",
    "#     line_segments.set_color(edge_colors)\n",
    "#     if keypoint_edges.shape[0]:\n",
    "#         line_segments.set_segments(keypoint_edges)\n",
    "#         line_segments.set_color(edge_colors)\n",
    "#     if keypoint_locs.shape[0]:\n",
    "#         scat.set_offsets(keypoint_locs)\n",
    "\n",
    "#     if crop_region is not None:\n",
    "#         xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "#         ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "#         rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "#         rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "#         rect = patches.Rectangle(\n",
    "#             (xmin,ymin),rec_width,rec_height,\n",
    "#             linewidth=1,edgecolor='b',facecolor='none')\n",
    "#         ax.add_patch(rect)\n",
    "\n",
    "#     fig.canvas.draw()\n",
    "#     image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "#     image_from_plot = image_from_plot.reshape(\n",
    "#       fig.canvas.get_width_height()[::-1] + (3,))\n",
    "#     plt.close(fig)\n",
    "#     if output_image_height is not None:\n",
    "#         output_image_width = int(output_image_height / height * width)\n",
    "#         image_from_plot = cv2.resize(\n",
    "#             image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "#              interpolation=cv2.INTER_CUBIC)\n",
    "#     return image_from_plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8778d1f-e392-4e38-b05b-a5893df39e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Cropping Algorithm\n",
    "\n",
    "# Confidence score to determine whether a keypoint prediction is reliable.\n",
    "MIN_CROP_KEYPOINT_SCORE = 0.2\n",
    "\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "    \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "    Args:\n",
    "        keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "          the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "        keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "        visualized.\n",
    "\n",
    "    Returns:\n",
    "        A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "          * the coordinates of all keypoints of all detected entities;\n",
    "          * the coordinates of all skeleton edges of all detected entities;\n",
    "          * the colors in which the edges should be plotted.\n",
    "    \"\"\"\n",
    "    keypoints_all = []\n",
    "    keypoint_edges_all = []\n",
    "    edge_colors = []\n",
    "    num_instances, _, _, _ = keypoints_with_scores.shape\n",
    "    for idx in range(num_instances):\n",
    "        kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "        kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "        kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "        kpts_absolute_xy = np.stack(\n",
    "            [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "        kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "            kpts_scores > keypoint_threshold, :]\n",
    "        keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "        for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "            if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "              kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "                x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "                y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "                x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "                y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "                line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "                keypoint_edges_all.append(line_seg)\n",
    "                edge_colors.append(color)\n",
    "    if keypoints_all:\n",
    "        keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "    else:\n",
    "        keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "    if keypoint_edges_all:\n",
    "        edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "    else:\n",
    "        edges_xy = np.zeros((0, 2, 2))\n",
    "    return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "    \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "        Args:\n",
    "        image: A numpy array with shape [height, width, channel] representing the\n",
    "          pixel values of the input image.\n",
    "        keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "          the keypoint coordinates and scores returned from the MoveNet model.\n",
    "        crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "          of the crop region in normalized coordinates (see the init_crop_region\n",
    "          function below for more detail). If provided, this function will also\n",
    "          draw the bounding box on the image.\n",
    "        output_image_height: An integer indicating the height of the output image.\n",
    "          Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "      Returns:\n",
    "        A numpy array with shape [out_height, out_width, channel] representing the\n",
    "        image overlaid with keypoint predictions.\n",
    "      \"\"\"\n",
    "    height, width, channel = image.shape\n",
    "    aspect_ratio = float(width) / height\n",
    "    fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "    # To remove the huge white borders\n",
    "    fig.tight_layout(pad=0)\n",
    "    ax.margins(0)\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    plt.axis('off')\n",
    "\n",
    "    im = ax.imshow(image)\n",
    "    line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "    ax.add_collection(line_segments)\n",
    "    # Turn off tick labels\n",
    "    scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "    (keypoint_locs, keypoint_edges,\n",
    "       edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "    if keypoint_edges.shape[0]:\n",
    "        line_segments.set_segments(keypoint_edges)\n",
    "        line_segments.set_color(edge_colors)\n",
    "    if keypoint_locs.shape[0]:\n",
    "        scat.set_offsets(keypoint_locs)\n",
    "\n",
    "    if crop_region is not None:\n",
    "        xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "        ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "        rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "        rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin,ymin),rec_width,rec_height,\n",
    "            linewidth=1,edgecolor='b',facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n",
    "    image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "    if output_image_height is not None:\n",
    "        output_image_width = int(output_image_height / height * width)\n",
    "        image_from_plot = cv2.resize(\n",
    "            image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "             interpolation=cv2.INTER_CUBIC)\n",
    "    return image_from_plot\n",
    "\n",
    "def torso_visible(keypoints):\n",
    "    \"\"\"Checks whether there are enough torso keypoints.\n",
    "\n",
    "    This function checks whether the model is confident at predicting one of the\n",
    "    shoulders/hips which is required to determine a good crop region.\n",
    "    \"\"\"\n",
    "    return ((keypoints[0, 0, KEYPOINT_DICT['left_hip'], 2] >\n",
    "             MIN_CROP_KEYPOINT_SCORE or\n",
    "            keypoints[0, 0, KEYPOINT_DICT['right_hip'], 2] >\n",
    "             MIN_CROP_KEYPOINT_SCORE) and\n",
    "            (keypoints[0, 0, KEYPOINT_DICT['left_shoulder'], 2] >\n",
    "             MIN_CROP_KEYPOINT_SCORE or\n",
    "            keypoints[0, 0, KEYPOINT_DICT['right_shoulder'], 2] >\n",
    "             MIN_CROP_KEYPOINT_SCORE))\n",
    "\n",
    "def determine_torso_and_body_range(\n",
    "    keypoints, target_keypoints, center_y, center_x):\n",
    "    \"\"\"Calculates the maximum distance from each keypoints to the center location.\n",
    "\n",
    "    The function returns the maximum distances from the two sets of keypoints:\n",
    "    full 17 keypoints and 4 torso keypoints. The returned information will be\n",
    "    used to determine the crop size. See determineCropRegion for more detail.\n",
    "    \"\"\"\n",
    "    torso_joints = ['left_shoulder', 'right_shoulder', 'left_hip', 'right_hip']\n",
    "    max_torso_yrange = 0.0\n",
    "    max_torso_xrange = 0.0\n",
    "    for joint in torso_joints:\n",
    "        dist_y = abs(center_y - target_keypoints[joint][0])\n",
    "        dist_x = abs(center_x - target_keypoints[joint][1])\n",
    "        if dist_y > max_torso_yrange:\n",
    "            max_torso_yrange = dist_y\n",
    "        if dist_x > max_torso_xrange:\n",
    "            max_torso_xrange = dist_x\n",
    "\n",
    "    max_body_yrange = 0.0\n",
    "    max_body_xrange = 0.0\n",
    "    for joint in KEYPOINT_DICT.keys():\n",
    "        if keypoints[0, 0, KEYPOINT_DICT[joint], 2] < MIN_CROP_KEYPOINT_SCORE:\n",
    "            continue\n",
    "        dist_y = abs(center_y - target_keypoints[joint][0]);\n",
    "        dist_x = abs(center_x - target_keypoints[joint][1]);\n",
    "        if dist_y > max_body_yrange:\n",
    "            max_body_yrange = dist_y\n",
    "\n",
    "        if dist_x > max_body_xrange:\n",
    "            max_body_xrange = dist_x\n",
    "\n",
    "    return [max_torso_yrange, max_torso_xrange, max_body_yrange, max_body_xrange]\n",
    "\n",
    "# def crop_and_resize(image, crop_region, crop_size):\n",
    "#     \"\"\"Crops and resize the image to prepare for the model input.\"\"\"\n",
    "#     boxes=[[crop_region['y_min'], crop_region['x_min'],\n",
    "#             crop_region['y_max'], crop_region['x_max']]]\n",
    "#     output_image = tf.image.crop_and_resize(\n",
    "#         image, box_indices=[0], boxes=boxes, crop_size=crop_size)\n",
    "#     return output_image\n",
    "\n",
    "# def run_inference(movenet, image, crop_region, crop_size):\n",
    "#     \"\"\"Runs model inferece on the cropped region.\n",
    "\n",
    "#     The function runs the model inference on the cropped region and updates the\n",
    "#     model output to the original image coordinate system.\n",
    "#     \"\"\"\n",
    "#     image_height, image_width, _ = image.shape\n",
    "#     input_image = crop_and_resize(\n",
    "#       tf.expand_dims(image, axis=0), crop_region, crop_size=crop_size)\n",
    "#     # Run model inference.\n",
    "#     keypoints_with_scores = movenet(input_image)\n",
    "#     # Update the coordinates.\n",
    "#     for idx in range(17):\n",
    "#         keypoints_with_scores[0, 0, idx, 0] = (\n",
    "#               crop_region['y_min'] * image_height +\n",
    "#               crop_region['height'] * image_height *\n",
    "#               keypoints_with_scores[0, 0, idx, 0]) / image_height\n",
    "#         keypoints_with_scores[0, 0, idx, 1] = (\n",
    "#               crop_region['x_min'] * image_width +\n",
    "#               crop_region['width'] * image_width *\n",
    "#               keypoints_with_scores[0, 0, idx, 1]) / image_width\n",
    "#     return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0608b993-87fe-4f09-8788-862dab2fa6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"movenet_thunder\" #@param [\"movenet_lightning\", \"movenet_thunder\", \"movenet_lightning_f16.tflite\", \"movenet_thunder_f16.tflite\", \"movenet_lightning_int8.tflite\", \"movenet_thunder_int8.tflite\"]\n",
    "model_name = \"movenet_thunder\"\n",
    "\n",
    "# Lightning\n",
    "if \"movenet_lightning\" in model_name:\n",
    "    input_size = 192\n",
    "    model_file = 'movenet_singlepose_lightning_4'        \n",
    "elif \"movenet_thunder\" in model_name:\n",
    "    model_file = 'movenet_singlepose_thunder_4'\n",
    "    input_size = 256\n",
    "else:\n",
    "    raise ValueError(\"Unsupported model name: %s\" % model_name)\n",
    "\n",
    "model = tf.saved_model.load(model_file)\n",
    "\n",
    "    \n",
    "def movenet(input_image):\n",
    "    \"\"\"Runs detection on an input image.\n",
    "\n",
    "    Args:\n",
    "      input_image: A [1, height, width, 3] tensor represents the input image\n",
    "        pixels. Note that the height/width should already be resized and match the\n",
    "        expected input resolution of the model before passing into this function.\n",
    "\n",
    "    Returns:\n",
    "      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n",
    "      coordinates and scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # model = tf.saved_model.load(model_file)\n",
    "\n",
    "    infer = model.signatures[\"serving_default\"]\n",
    "\n",
    "    # SavedModel format expects tensor type of int32.\n",
    "    input_image = tf.cast(input_image, dtype=tf.int32)\n",
    "    # Run model inference.\n",
    "    outputs = infer(input_image)\n",
    "    # Output is a [1, 1, 17, 3] tensor.\n",
    "    keypoints_with_scores = outputs['output_0'].numpy()\n",
    "    return keypoints_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12237dcd-eb4e-462c-8884-3bd75fe7e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bounding_box(keypoints_with_scores, height, width, input_size, keypoint_threshold=0.0):\n",
    "    kpts_x = keypoints_with_scores[0, 0, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, 0, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, 0, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[kpts_scores > keypoint_threshold, :]\n",
    "    kpts_x = kpts_above_thresh_absolute[:, 1]\n",
    "    kpts_y = kpts_above_thresh_absolute[:, 0]\n",
    "    x_min = kpts_x.min() - 10 \n",
    "    y_min = kpts_y.min() - 10\n",
    "    x_max = kpts_x.max() + 10\n",
    "    y_max = kpts_y.max() + 10\n",
    "\n",
    "    return {\n",
    "      'y_min': y_min,\n",
    "      'x_min': x_min,\n",
    "      'y_max': y_max,\n",
    "      'x_max': x_max,\n",
    "      'height': y_max - y_min,\n",
    "      'width': x_max - x_min\n",
    "    }\n",
    "    \n",
    "    \n",
    "def inference(image_path):\n",
    "    start = time.time()\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image)\n",
    "    image_height, image_width, _ = image.shape\n",
    "    input_image = tf.expand_dims(image, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)\n",
    "    # print(f'keypoints_with_scores: {keypoints_with_scores}')\n",
    "    \n",
    "    # Get ideal crop region.\n",
    "    # crop_region = get_bounding_box(\n",
    "      # keypoints_with_scores, image_height, image_width, input_size)\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(image, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(output_overlay)\n",
    "    _ = plt.axis('off')\n",
    "    # plot_bounding_box(crop_region)\n",
    "    plt.show()\n",
    "    \n",
    "    # Show just the pose\n",
    "    \n",
    "    skeleton = draw_prediction_on_image(\n",
    "        np.squeeze(np.zeros(display_image.shape), axis=0), keypoints_with_scores)\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(skeleton)\n",
    "    _ = plt.axis('off')\n",
    "    # plot_bounding_box(crop_region)\n",
    "    plt.show()\n",
    "\n",
    "    end = time.time()\n",
    "    print(f'Time taken: {end - start}')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20c9dd21-6220-4de4-841d-cf284b8bf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference('input_image.jpeg')\n",
    "# inference('img_13.jpg')\n",
    "# inference('img_24.jpg')\n",
    "# inference('img_28.jpg')\n",
    "# inference('img_36.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0218eb5e-01e0-4dce-9a68-cac9d0c85c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 2 µs, total: 6 µs\n",
      "Wall time: 11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e95b98-cac3-4fe1-b3b0-c37bbe00002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c0 files...\n",
      "Processed 0:0 files...\n",
      "Extracted from 0 files...\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_94962_keypoints.pt\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_50920_keypoints.pt\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_41839_keypoints.pt\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_31460_keypoints.pt\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_37011_keypoints.pt\n",
      "Keypoints saved to /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/tt/c0/img_1487_keypoints.pt\n",
      "Total image: 6, time taken:13.142754999999994\n",
      "CPU times: user 12 s, sys: 1.11 s, total: 13.1 s\n",
      "Wall time: 7.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# extract_poses(config, pose_config.FEATURES_FOLDER_FULL, pose_config.SUMMARY_NAME, limit=800)\n",
    "extract_poses(config, f'{config.OUTPUT_FOLDER}/tt', pose_config.SUMMARY_NAME, limit=5)\n",
    "# cProfile.run(\n",
    "# \"extract_poses(config, f'{config.OUTPUT_FOLDER}/tt', pose_config.SUMMARY_NAME, limit=5)\", f'{config.OUTPUT_FOLDER}/mtcnn_stats_model_load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e28546ae-261f-4fde-9123-b2f450cca0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 30 06:47:25 2022    /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/mtcnn_stats\n",
      "\n",
      "         98601988 function calls (97685069 primitive calls) in 125.128 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 2741 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "   2545/1    0.161    0.000  125.137  125.137 {built-in method builtins.exec}\n",
      "        1    0.012    0.012  125.137  125.137 <timed exec>:1(extract_poses)\n",
      "        6    0.070    0.012  122.591   20.432 /var/folders/s4/p0y554f56hd_d9fk8y79dy3c0000gs/T/ipykernel_26988/826767389.py:67(get_pose)\n",
      "        6    0.000    0.000  116.908   19.485 /var/folders/s4/p0y554f56hd_d9fk8y79dy3c0000gs/T/ipykernel_26988/2596138246.py:14(movenet)\n",
      "        6    0.003    0.000  110.245   18.374 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:707(load)\n",
      "        6    0.000    0.000  110.243   18.374 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:804(load_partial)\n",
      "        6    0.017    0.003  109.085   18.181 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:145(__init__)\n",
      "        6    0.800    0.133   90.246   15.041 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:319(load_function_def_library)\n",
      "     5826    1.502    0.000   66.889    0.011 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/framework/function_def_to_graph.py:34(function_def_to_graph)\n",
      "     5826    0.021    0.000   46.454    0.008 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/tensorflow/python/framework/importer.py:411(import_graph_def_for_function)\n",
      "\n",
      "\n",
      "Wed Nov 30 06:50:35 2022    /Users/rasentha/mids/w281/project/w281-fall2022-section2-team3/output/mtcnn_stats_model_load\n",
      "\n",
      "         1008757 function calls (986687 primitive calls) in 8.062 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 2478 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "     12/1    0.000    0.000    8.063    8.063 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    8.063    8.063 <string>:1(<module>)\n",
      "        1    0.010    0.010    8.063    8.063 <timed exec>:1(extract_poses)\n",
      "        6    0.029    0.005    5.621    0.937 /var/folders/s4/p0y554f56hd_d9fk8y79dy3c0000gs/T/ipykernel_34397/826767389.py:67(get_pose)\n",
      "       12    0.001    0.000    3.510    0.292 /var/folders/s4/p0y554f56hd_d9fk8y79dy3c0000gs/T/ipykernel_34397/472624911.py:104(draw_prediction_on_image)\n",
      "       12    0.000    0.000    2.072    0.173 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:429(draw)\n",
      "       12    0.000    0.000    2.066    0.172 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/matplotlib/artist.py:72(draw_wrapper)\n",
      "   120/12    0.005    0.000    2.066    0.172 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/matplotlib/artist.py:33(draw_wrapper)\n",
      "       12    0.000    0.000    2.066    0.172 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/matplotlib/figure.py:2821(draw)\n",
      "    24/12    0.000    0.000    2.045    0.170 /Users/rasentha/opt/anaconda3/envs/amd-gpu2/lib/python3.8/site-packages/matplotlib/image.py:114(_draw_list_compositing_images)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x7fdcb4898ac0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pstats\n",
    "# from pstats import SortKey\n",
    "# p1 = pstats.Stats(f'{config.OUTPUT_FOLDER}/mtcnn_stats')\n",
    "# # p.strip_dirs().sort_stats(-1).print_stats()\n",
    "# p1.sort_stats(SortKey.CUMULATIVE).print_stats(10)\n",
    "# # p.sort_stats(SortKey.TIME).print_stats(10)\n",
    "\n",
    "# p2 = pstats.Stats(f'{config.OUTPUT_FOLDER}/mtcnn_stats_model_load')\n",
    "# p2.sort_stats(SortKey.CUMULATIVE).print_stats(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e576652-e1ad-4368-8b2c-f0fb3430a7ad",
   "metadata": {},
   "source": [
    "# 3. Post Processing Of Poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886b02cd-214f-4ea2-ae8e-e29f46082741",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_save_pose(orig_pose_file, cropped_pose_file, verbose=False):\n",
    "    # Read the gray scale image.\n",
    "    img = cv2.imread(orig_pose_file, cv2.IMREAD_GRAYSCALE)\n",
    "    PADDING = 20\n",
    "    if verbose:\n",
    "        plt.title('Original (grayscale)')\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    # Quick fix for removing the white border that the pose isolation code adds.\n",
    "    zero_pixels = np.argwhere(img == 0)\n",
    "    border_top = np.min(zero_pixels[:,0])\n",
    "    border_left  = np.min(zero_pixels[:,1])\n",
    "    border_bottom = np.max(zero_pixels[:,0])\n",
    "    border_right = np.max(zero_pixels[:,1])\n",
    "    img = img[border_top:border_bottom, border_left:border_right]\n",
    "    if verbose:\n",
    "        plt.title('Border removed')\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "    # Find the crop region, accounting for the white border we just removed.\n",
    "    positions = np.argwhere(img > 0)\n",
    "    top = np.min(positions[:,0]) + border_top\n",
    "    left  = np.min(positions[:,1]) + border_left\n",
    "    bottom = np.max(positions[:,0]) + border_top\n",
    "    right = np.max(positions[:,1]) + border_left\n",
    "    d = max(right - left, bottom - top)\n",
    "    padding_x = d - (bottom - top)\n",
    "    padding_y = d - (right - left)\n",
    "    if verbose:\n",
    "        print(f'top:{top}, left:{left}, bottom:{bottom}, right:{right}, d:{d}')\n",
    "    \n",
    "    # Re-read the color image and apply the crop\n",
    "    img = cv2.imread(orig_pose_file)\n",
    "    crop_img = img[top:bottom, left:right]\n",
    "    if verbose:\n",
    "        plt.title('Cropped (grayscale)')\n",
    "        plt.imshow(crop_img)\n",
    "        plt.show()\n",
    "    \n",
    "    # Add padding.\n",
    "    crop_img = cv2.copyMakeBorder(crop_img, PADDING, PADDING + padding_x, PADDING, padding_y + PADDING, cv2.BORDER_CONSTANT, None, 0)\n",
    "    crop_img = cv2.resize(crop_img, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "    if verbose:\n",
    "        plt.title('Padded and resized')\n",
    "        plt.imshow(crop_img)\n",
    "        plt.show()\n",
    "    cv2.imwrite(cropped_pose_file, crop_img)\n",
    "    \n",
    "def post_processing(labels, pose_config, reduced=False, limit=10):\n",
    "    total_images = 0\n",
    "    start = time.process_time()\n",
    "\n",
    "    for label in labels:\n",
    "        if limit is not None:\n",
    "            if total_images > limit:\n",
    "                break\n",
    "        print(f'Processing c{label} files...')\n",
    "        dataset = customdataset.DriverDataset(config, reduced=reduced, label=label)\n",
    "        dataloader = DataLoader(dataset, num_workers=0, batch_size=1, shuffle=False, collate_fn=dataset.get_image_from)\n",
    "        for batch_idx, samples in enumerate(dataloader):\n",
    "            total_images = total_images + 1\n",
    "            # summary = []\n",
    "            if batch_idx % 1000 == 0:\n",
    "                print(f'Processed {label}:{batch_idx} files...')\n",
    "                print(f'Extracted from {total_images} files...')\n",
    "            if limit is not None:\n",
    "                if total_images > limit:\n",
    "                    break\n",
    "            _, label, orig_filename = samples\n",
    "            filename = os.path.basename(orig_filename)\n",
    "            file_primary_name, _ = os.path.splitext(os.path.basename(filename))\n",
    "            orig_pose_file = f'{pose_config.FEATURES_FOLDER_FULL}/c{label}/{file_primary_name}_pose.png'\n",
    "            cropped_pose_file = f'{pose_config.FEATURES_FOLDER_FULL}/c{label}/{file_primary_name}_pose_cropped.png'            \n",
    "            # Open the file and crop it to just the pose + a border of 5 pixels and then save it.\n",
    "            # Resize it?\n",
    "            process_and_save_pose(orig_pose_file, cropped_pose_file)\n",
    "    \n",
    "post_processing(config.included_labels, pose_config, limit=None)\n",
    "# post_processing([0], pose_config, limit=10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2190e121-ebc6-4e6e-82e3-faedf3809369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdbfb62-3865-4bf3-8e42-440cbbceb685",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
